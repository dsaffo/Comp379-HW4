{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Saffo\n",
    "## Comp379-HW4\n",
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in two data files, splitting them line by line into a list, merging them into one list, labeling them pos or neg, shuffling the indexes, and splittin the data 70,15,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10664\n",
      "7465\n",
      "1600\n",
      "1599\n"
     ]
    }
   ],
   "source": [
    "goodFile = open(\"rt-polarity.pos\",\"r\")\n",
    "goodReviews = goodFile.read()\n",
    "goodReviews = goodReviews.lower()\n",
    "goodReviews = goodReviews.split(\"\\n\")\n",
    "\n",
    "badFile = open(\"rt-polarity.neg\",\"r\")\n",
    "badReviews = badFile.read()\n",
    "badReviews = badReviews.lower()\n",
    "badReviews = badReviews.split(\"\\n\")\n",
    "\n",
    "ratings = []\n",
    "for i in range(0, len(goodReviews)):\n",
    "    ratings.append('pos')\n",
    "for i in range(0, len(badReviews)):\n",
    "    ratings.append('neg')\n",
    "\n",
    "reviews = []\n",
    "reviews = goodReviews + badReviews\n",
    "review = []\n",
    "for i in range(0, len(reviews)):   \n",
    "    if ratings[i] == 'pos':\n",
    "        category = \"pos\"\n",
    "        value = reviews[i]\n",
    "    else: \n",
    "        category = \"neg\"\n",
    "        value = reviews[i]\n",
    "        \n",
    "    review.append((category, value))\n",
    "\n",
    "print (len(review))\n",
    "\n",
    "random.shuffle(review)\n",
    "\n",
    "split = round(len(review)*.70)\n",
    "split2 = round(len(review)*.15)\n",
    "\n",
    "train = review[0:split]\n",
    "del review[0:split]\n",
    "dev = review[0:split2]\n",
    "del review[0:split2]\n",
    "test = review[0:split2]\n",
    "\n",
    "print (len(train))\n",
    "print (len(dev))\n",
    "print (len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for removing punctuation and triming the whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeP(data):\n",
    "    table = data.translate(str.maketrans(\"\", \"\", string.punctuation)) \n",
    "    return table\n",
    "\n",
    "def Trim2(data):\n",
    "    data = removeP(data)\n",
    "    data = data.lower()\n",
    "    return re.split(\"\\W+\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the trim function and counter method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'to': 2, 'going': 1, '': 1, 'destined': 1, 'than': 1, 'or': 1, 'arnold': 1, 'schwarzenegger': 1, 'new': 1, 'van': 1, 'greater': 1, 'a': 1, 'segal': 1, 'conan': 1, 'be': 1, 'and': 1, 'damme': 1, '21st': 1, 'jeanclaud': 1, 'steven': 1, 'rock': 1, 'is': 1, 'make': 1, 'centurys': 1, 'splash': 1, 'even': 1, 'hes': 1, 'that': 1})\n"
     ]
    }
   ],
   "source": [
    "a = Trim2(goodReviews[0])\n",
    "c = Counter(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model by counting words and their prior apperances/labels\n",
    "###### Implementation learned here : http://blog.yhat.com/posts/naive-bayes-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag = {}\n",
    "Wcount = {\"pos\": {}, \"neg\": {}}\n",
    "prior = {\"pos\" : 0., \"neg\" : 0.}\n",
    "docs = []\n",
    "\n",
    "for i in range(0, len(train)):   \n",
    "    category = train[i][0]\n",
    "    value = train[i][1]\n",
    "        \n",
    "    docs.append((category, value))\n",
    "    \n",
    "    prior[category] += 1\n",
    "    \n",
    "    words = Trim2(value)\n",
    "    \n",
    "    counts = Counter(words)\n",
    "    \n",
    "    for word, count in counts.items():\n",
    "        if word not in bag:\n",
    "            bag[word] = 0.0 \n",
    "        if word not in Wcount[category]:\n",
    "            Wcount[category][word] = 0.0\n",
    "        bag[word] += count\n",
    "        Wcount[category][word] += count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for label prediction, takes reviews one at a time and uses the naive bayes algorithm to predict labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Predict(data):\n",
    "    scores = []\n",
    "    for i in range(0, len(data)):\n",
    "        words = Trim2(data[i][1])\n",
    "        counts = Counter(words)\n",
    "    \n",
    "        prior_pos = (prior[\"pos\"] / sum(prior.values()))\n",
    "        prior_neg = (prior[\"neg\"] / sum(prior.values()))\n",
    "\n",
    "        log_pos = 0.0\n",
    "        log_neg = 0.0\n",
    "\n",
    "        for w, count in counts.items():\n",
    "            if not w in bag or len(w) <=2:\n",
    "                continue\n",
    "            p_word = bag[w] / sum(bag.values())\n",
    "            p_w_give_pos = Wcount[\"pos\"].get(w, 0.0) / sum(Wcount[\"pos\"].values())\n",
    "            p_w_give_neg = Wcount[\"neg\"].get(w, 0.0) / sum(Wcount[\"neg\"].values())\n",
    "    \n",
    "            if p_w_give_pos > 0:\n",
    "                log_pos += math.log(count * p_w_give_pos / p_word)\n",
    "            if p_w_give_neg > 0:\n",
    "                log_neg += math.log(count * p_w_give_neg / p_word)\n",
    "        \n",
    "        Pscore = math.exp(log_pos + math.log(prior_pos))\n",
    "        Nscore = math.exp(log_neg + math.log(prior_neg))                  \n",
    "    \n",
    "        if Pscore > Nscore:\n",
    "            scores.append(\"pos\")\n",
    "        else:\n",
    "            scores.append(\"neg\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takes list of predicted labels from above and runs accuracey and f1score tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.76125\n",
      "f1 score:  0.757306226175\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "target2 = []\n",
    "predictions2 = []\n",
    "for l in range(0, len(dev)):\n",
    "    target.append(dev[l][0])\n",
    "predictions = Predict(dev)\n",
    "\n",
    "for l in range(0, len(dev)):\n",
    "    if target[l] == 'pos':\n",
    "        target2.append(1)\n",
    "    else:\n",
    "        target2.append(0)\n",
    "\n",
    "for l in range(0, len(dev)):\n",
    "    if predictions[l] == 'pos':\n",
    "        predictions2.append(1)\n",
    "    else:\n",
    "        predictions2.append(0)        \n",
    "\n",
    "print(\"accuracy: \" , accuracy_score(target, predictions))\n",
    "print(\"f1 score: \" , f1_score(target2, predictions2, average = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above but for test set for final scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.775484677924\n",
      "f1 score:  0.770607028754\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "target2 = []\n",
    "predictions2 = []\n",
    "for l in range(0, len(test)):\n",
    "    target.append(test[l][0])\n",
    "\n",
    "predictions = Predict(test)\n",
    "\n",
    "for l in range(0, len(test)):\n",
    "    if target[l] == 'pos':\n",
    "        target2.append(1)\n",
    "    else:\n",
    "        target2.append(0)\n",
    "\n",
    "for l in range(0, len(test)):\n",
    "    if predictions[l] == 'pos':\n",
    "        predictions2.append(1)\n",
    "    else:\n",
    "        predictions2.append(0)        \n",
    "\n",
    "print(\"accuracy: \" , accuracy_score(target, predictions))\n",
    "print(\"f1 score: \" , f1_score(target2, predictions2, average = 'binary'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
